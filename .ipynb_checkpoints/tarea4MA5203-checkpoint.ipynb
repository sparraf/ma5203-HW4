{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea #4: MA5203 Aprendizaje de MÃ¡quinas ProbabilÃ­stico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Redes Neuronales y MNIST (75%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tarea deberÃ¡n implementar una __red neuronal convolucional__ (CNN) profunda para resolver el problema de clasificaciÃ³n de las imÃ¡genes de dÃ­gitos escritos a mano (MNIST), en donde se espera que alcancen mayores niveles de accuracy que con la red neuronal feedforward mostrada en el tutorial de TensorFlow.\n",
    "\n",
    "Para esto:\n",
    "> + DiseÃ±arÃ¡n la arquitectura de una red convolucional <br>\n",
    "> + AjustarÃ¡n un algoritmo de optimizaciÃ³n de su preferencia <br>\n",
    "> + RegularizarÃ¡n la red neuronal <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1) Instrucciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debe entregar el Jupyter Notebook completando en el cÃ³digo las secciones indicadas y seÃ±alizadas especÃ­ficamente por (...). Puede agregar o quitar partes del diseÃ±o de la arquitectura como estime conveniente. TambiÃ©n deberÃ¡ entregar un informe de __mÃ¡ximo 2 planas__ que incluya:\n",
    "\n",
    "> + Arquitectura de la red <br>\n",
    "> + Ajuste de hiperparÃ¡metros y optimizador elegido<br>\n",
    "> + RegularizaciÃ³n y entrenamiento <br>\n",
    "> + PresentaciÃ³n de resultados <br>\n",
    "> + AnÃ¡lisis de resultados y conclusiones <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Fase de construcciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera funciÃ³n a construir es _deepnn()_, que construye el grafo con la arquitectura de la red convolucional con la que se resolverÃ¡ el problema. Esta debe contener la siguiente estructura (como mÃ­nimo) dentro de los _name scopes_:\n",
    "+ 'reshape': haz un reshape al input para usarlo dentro de una red convolucional (https://www.tensorflow.org/api_docs/python/tf/reshape) <br>\n",
    "+ capa de convoluciÃ³n (se recomienda mÃ¡s de 1):\n",
    "> + 'conv': capa de convoluciÃ³n <br>\n",
    "> + 'pool': capa de pooling <br>\n",
    "> + 'fc': capa fully connected <br>\n",
    "+ 'droput': mÃ¡scara de dropout para regularizaciÃ³n (puede usar otra tÃ©cnica) (https://www.tensorflow.org/api_docs/python/tf/nn/dropout) <br>\n",
    "+ 'cl_layer': capa de clasificaciÃ³n <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "    \"\"\"\n",
    "  FunciÃ³n que construye el grafo de una red profunda\n",
    "  \n",
    "  --------PUEDE AGREGAR O QUITAR ELEMENTOS DE LA CAPA PREDEFINIDA MÃS ADELANTE------\n",
    "  \n",
    "  Args:\n",
    "    x: tensor de input con dimensiones (N_examples, 784)\n",
    "  Returns:\n",
    "    Una tupla (y, keep_prob), con y un tensor de dimensiÃ³n (N_examples, 10),\n",
    "    con los valores de los logits de clasificar el dÃ­gito en una de las 10 classes,\n",
    "    y keep_prob un escalar placeholder para la probabilidad de dropout.\n",
    "  \"\"\"\n",
    "  \n",
    "  ####----------------- (1) COMPLETAR ---------------------#####\n",
    "    \n",
    "    # Reshape para usar input en una red convolucional (hint: se debe hacer un 'flatten' a 1D)\n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, [..., ..., ..., ...])\n",
    "        \n",
    "  ####---------------- fin completar -----------------#####\n",
    "\n",
    "  ####----------------- (2) COMPLETAR ---------------------#####\n",
    "    \n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([..., ..., ..., ...])\n",
    "        b_conv1 = bias_variable([...])\n",
    "        # Ver el bloque de mÃ¡s abajo para las funciones conv2d, max_pool_2x2,\n",
    "        # weight_variable, y bias_variable\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "    with tf.name_scope('pool1'):\n",
    "        h_pool1 = max_pool_2x2(...)\n",
    "\n",
    "    with tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([..., ..., ..., ...])\n",
    "        b_conv2 = bias_variable([...])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    with tf.name_scope('pool2'):\n",
    "        h_pool2 = max_pool_2x2(...)\n",
    "\n",
    "    with tf.name_scope('fc1'):\n",
    "        W_fc1 = weight_variable([..., ...])\n",
    "        b_fc1 = bias_variable([...])\n",
    "\n",
    "        # El argumento -1 hace 'flatten' a 1D\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, ...])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        \n",
    "  ####---------------- fin completar -----------------#####\n",
    "\n",
    " ####----------------- (3) COMPLETAR ---------------------#####\n",
    "    \n",
    "    # Implementar regularizaciÃ³n por dropout\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = ...\n",
    "        \n",
    "  ####---------------- fin completar -----------------#####\n",
    "\n",
    " ####----------------- (4) COMPLETAR ---------------------#####\n",
    "    \n",
    "  # Map un nÃºmero de features al nÃºmero de clases\n",
    "    with tf.name_scope('cl_layer'):\n",
    "        W_fc2 = weight_variable([..., ...])\n",
    "        b_fc2 = bias_variable([...])\n",
    "\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return y_conv, keep_prob\n",
    "        \n",
    "  ####---------------- fin completar -----------------#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaciÃ³n se definen funciones que le serÃ¡n Ãºtiles para la arquitectura definida en deep_nn():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    \"\"\"conv2d retorna una capa de convoluciÃ³n 2D con full stride\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"max_pool_2x2 hace downsample al feature map por 2X\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    " ####----------------- (5) COMPLETAR ---------------------##### \n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"weight_variable genera una variable de peso dadas ciertas dimensiones\"\"\"\n",
    "    initial = tf.truncated_normal(..., stddev=...)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"bias_variable genera una variable debias variable dado cierto shape\"\"\"\n",
    "    initial = tf.constant(..., shape=...)\n",
    "    return tf.Variable(initial)\n",
    "        \n",
    "  ####---------------- fin completar -----------------#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Fase de ejecuciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa, antes de ejecutar la sesiÃ³n deberÃ¡ definir los name scopes de:\n",
    "+ 'loss': funciÃ³n de costos a optimizar <br>\n",
    "+ 'my_optimizer': algoritmo de optimizaciÃ³n e hiperparÃ¡metros que utilizarÃ¡ en el entrenamiento <br>\n",
    "Algunas opciones (se valorizarÃ¡ el uso de un algoritmo distinto al mostrado en el tutorial):\n",
    "> + __Descenso por el gradiente__, __descenso estocÃ¡stico por el gradiente__, __descenso por el gradiente con mini-batches__: https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer <br>\n",
    "> + __RMSProp__ (algoritmo con momentum): https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer <br>\n",
    "> + __Adagrad__ (algoritmo con learning rates adaptativos): https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer <br>\n",
    "> + __Adam__ (algoritmo con momentum y learning rates adaptativos): https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer <br>\n",
    "+ 'accuracy': mÃ©trica de desempeÃ±o\n",
    "\n",
    "__Tenga en consideraciÃ³n que la siguiente etapa puede tomar una cantidad significativa de tiempo (3 min ~ 1,000 epochs)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FunciÃ³n que realiza la fase de ejecuciÃ³n\n",
    "def main(mnist_data):\n",
    "    # Importar datos\n",
    "    mnist = input_data.read_data_sets(FLAGS.data_dir)\n",
    "\n",
    " ####----------------- (6) COMPLETAR ---------------------#####\n",
    "    \n",
    "    # Inputs, etiquetas, y output de la red convolucional\n",
    "    x = tf.placeholder(tf.float32, [..., ...])\n",
    "\n",
    "    y_ = tf.placeholder(tf.int64, [...])\n",
    "\n",
    "    y_conv, keep_prob = ...\n",
    "        \n",
    "  ####---------------- fin completar -----------------#####\n",
    "\n",
    " ####----------------- (7) COMPLETAR ---------------------#####\n",
    "    \n",
    "    # FunciÃ³n objetivo, optimizador y evaluaciÃ³n\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = ...\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('my_optimizer'):\n",
    "        train_step = ...\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), y_)\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = ...\n",
    "        \n",
    "  ####---------------- fin completar -----------------#####\n",
    "\n",
    "    graph_location = tempfile.mkdtemp()\n",
    "    print('Saving graph to: %s' % graph_location)\n",
    "    train_writer = tf.summary.FileWriter(graph_location)\n",
    "    train_writer.add_graph(tf.get_default_graph())\n",
    "\n",
    " ####----------------- (8) COMPLETAR ---------------------#####\n",
    "    \n",
    "    # Ejecutar la sesiÃ³n que entrena la red convolucional\n",
    "    with tf.Session() as sess:\n",
    "        #Inicialice las variables\n",
    "        ...\n",
    "        for i in range(...):\n",
    "            X_batch, y_batch = mnist.train.next_batch(...)\n",
    "            # Imprimir mÃ©tricas cada 100 epochs\n",
    "            if i % 100 == 0:\n",
    "                train_accuracy = ...\n",
    "                print('step %d, training set accuracy %g' % (i, train_accuracy))\n",
    "                validation_accuracy = ...\n",
    "                print('validation set accuracy %g' % validation_accuracy)\n",
    "                test_accuracy = ...\n",
    "                print('test set accuracy %g' % test_accuracy)\n",
    "            # El argumento keep_prob indica la probabilidad de mantener un nodo de input\n",
    "            # (es dropout aplicado al input)\n",
    "            train_step.run(feed_dict={x: ..., y_: ..., keep_prob: ...})\n",
    "        \n",
    "  ####---------------- fin completar -----------------#####\n",
    "            \n",
    "# Esta parte del cÃ³digo ejecuta la funciÃ³n main(). No necesitas modificarla\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str,\n",
    "                      default='/tmp/tensorflow/mnist/input_data',\n",
    "                      help='Directory for storing input data')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4) Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestre curvas de aprendizaje (e.g., pÃ©rdida en funciÃ³n de Ã©pocas), comparaciÃ³n entre los distintos sets, matrices de confusiÃ³n y anÃ¡lisis de error en el resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto (25%)\n",
    "\n",
    "Defina sucintamente el objetivo de su proyecto, enmÃ¡rquelo en alguna de las temÃ¡ticas del curso (regresiÃ³n, clasificaciÃ³n, clustering, reducciÃ³n de dimensionalidad, etc.) e identifique sus datos con la notaciÃ³n vista en clase (e.g., defina sus _inputs_ y _labels_ si su problema es de clasificaciÃ³n). AdemÃ¡s, explore las herramientas clÃ¡sicas para resolver su problema e implemente al menos una con sus datos. \n",
    "\n",
    "ExtensiÃ³n mÃ¡xima para esta parte: **2 pÃ¡ginas**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
