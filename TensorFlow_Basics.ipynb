{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSORFLOW BASICS ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "MA5203 Aprendizaje de Máquinas Probabilístico\n",
    "<br>\n",
    "Autor: Nicolás Aramayo\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow es una librería open source para computación numérica de alto desempeño, especialmente diseñado para Machine Learning de gran escala. Su arquitectura flexible permite el despliegue de computación a través de múltiples plataformas (CPUs, GPUs, TPUs), y desde estacionarios a clusters de servidores a dispositivos móviles. Fue originalmente desarrollado por investigadores e ingenieros de Google Brain dentro de la organización de AI de Google, y tiene un fuerte soporte para machine learning y deep learning.\n",
    "\n",
    "https://www.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar TensorFlow, se deben cumplir los siguientes requerimientos:<br>\n",
    "> -64-bit, x86 estacionarios o laptops\n",
    "<br>\n",
    "> -Windows 7 o más\n",
    "\n",
    "Si Python fue instalado usando Anaconda, se recomienda abrir el Prompt Anaconda y ejecutar: pip install --upgrade tensorflow\n",
    "<br>(Esto se hace para instalar TensorFlow en un virtual environment)\n",
    "\n",
    "Más información: https://www.tensorflow.org/install/\n",
    "<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print 'Hello, TensorFlow!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Un programa de TensorFlow típicamente se divide en dos partes: primero construye un grafo computacional (__fase de construcción__), y luego ejecútalo (__fase de ejecución__). Su forma de trabajar es simple, primero definiendo un grafo de computaciones en Python, y luego TensorFlow toma este grafo para ejecutarlo en C++.\n",
    "\n",
    "-__Fase de construcción__: construye un grafo computacional representando al modelo de Machine Learning y las computaciones necesarias para entrenarlo.\n",
    "\n",
    "-__Fase de ejecución__: ejecuta un loop que evalúa un training step repetidamente, gradualmente mejorando los parámetros del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea un Grafo Computacional y Ejecútalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, debes abrir una __sesión__ de TensorFlow. Las sesiones se hacen cargo de llevar las operaciones a dispositivos (CPUs, GPUs) y ejecutarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "#Inicializa las variables\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "\n",
    "#Evalúa\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "\n",
    "#Cierra la sesión\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente imagen muestra el grafo computacional recién implementado. Toma como inputs los nodos _x_ e _y_ y aplica las operaciones detalladas en _f_ para generar el output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/nicoaramayo/imgsTFBasics/raw/master/grafo_1.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma más simple de ejecutar una sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    print(result)\n",
    "#Esta sesión fue cerrada automáticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mejor aún, inicializa automáticamente todas las variables creando un nodo 'init'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en el ejemplo, una __variable__ en TensorFlow se añade al grafo computacional al construir una instancia de la clase Variable. El constructor Variable() necesita un valor inicial para la variable:\n",
    "\n",
    "                                w = tf.Variable(<valor-inicial>, name=<nombre-opcional>)\n",
    "                         \n",
    "Para cambiar el valor de la variable se puede usar el método __assign()__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejando Grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cualquier nodo creado se añade automáticamente al grafo default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para manejar múltiples grafos independientes, crea un nuevo __Grafo__ y temporalmente ejecútalo como el grafo default dentro de un bloque _with_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Jupyter, ejecutar una misma celda más de una vez puede resultar en que un grafo default contenga muchos nodos duplicados. Una solución es reiniciar el _kernel_, o mejor aún, reiniciar el grafo default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al evaluar un nodo, TensorFlow automáticamente determinará el set de nodos de los cuales depende y evaluará estos nodos primero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los valores en los nodos se eliminan entre ejecuciones del grafo excepto los valores variables, los cuales son mantenidos por la sesión entre ejecuciones del grafo. Una variable comienza su vida cuando se ejecuta el inicializador, y termina cuando la sesión es cerrada\n",
    "\n",
    "Para evaluar de manera eficiente _y_ y _z_, pídele a TensorFlow que los evalúe ambos en una misma ejecución del grafo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1: Implementación de Descenso del Gradiente en una Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa el dataset California Housing, que contiene 8 atributos sobre casas (contenidos en _housing.data_) y la valoración de la casa como la variable dependiente (contenido en _housing.target_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "#Obtiene las dimensiones de la matriz de covariables X\n",
    "m, n = housing.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al usar Descenso del Gradiente es importante normalizar los vectores de input feature, o de otra forma el entrenamiento podría ser mucho más lento. Piensa en descender por una tazón alargado versus descender por un tazón con una curvatura simétrica: el camino por cubrir podría ser mucho más corto!\n",
    "\n",
    "Para realizar esto se usará la función _StandardScaler_ de _sklearn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "#Añade el bias\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define el número de iteraciones y el learning rate\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "#Define X e y como constantes de TensorFlow, y theta como una variable de TensorFlow ya que será constantemente actualizada\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "#Obtén los valores iniciales de manera aleatoria, con las dimensiones correctas\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "#Guardamos los valores del MSE en una lista\n",
    "costs = []\n",
    "\n",
    "#Usa el optimizador Gradient Descent implementado en TensorFlow, y entrena para optimizar el mean squared error\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        costs.append(mse.eval())\n",
    "    \n",
    "    #Recupera el theta optimizado\n",
    "    best_theta = theta.eval()\n",
    "    print(\"MSE óptimo luego de \" + str(n_epochs) + \" iteraciones:\")\n",
    "    print(mse.eval())\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)\n",
    "\n",
    "#Obtiene el óptimo analítico para comparación\n",
    "X_ols = scaled_housing_data_plus_bias\n",
    "y_ols = housing.target.reshape(-1, 1)\n",
    "y_pred_ols = np.matmul(X_ols, np.linalg.lstsq(X_ols, y_ols)[0])\n",
    "mse_ols = np.mean(np.square(y_pred_ols - y_ols))\n",
    "mse_in_time = [mse_ols] * n_epochs\n",
    "\n",
    "plt.plot(np.squeeze(costs), 'b')\n",
    "plt.plot(mse_in_time, 'r')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1.1: Implementa Descenso del Gradiente por Mini-Batches para una Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para Descenso del Gradiente por Mini-Batches, _X_ e _y_ deben ser reemplazados en cada iteración con el siguiente mini-batch. Esto se puede realizar usando nodos __placeholder__. Estos nodos no realizan ninguna operación, solo entregan como output la data que tú les pides que entreguen durante la ejecución. Típicamente son usados para entregar la data de entrenamiento a TensorFlow durante el entrenamiento. Para la evaluación, usa __feed_dict__ en el método _eval()_ durante el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#Define X e y como nodos placeholder\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "costs = []\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Elige el tamaño del batch para realizar Descenso del Gradiente por Mini-Batches,\n",
    "#y la cantidad de batches será el número de filas sobre el tamaño de los batches\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    '''\n",
    "    Función para obtener batches de X e y de tamaño batch_size\n",
    "    '''\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    #Obtén una cantidad batch_size de índices de 0 a m-1\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    #Elige en X e y las filas de acuerdo con los índices sampleados\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        for batch_index in range(n_batches):\n",
    "            \n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            #Para evaluar, entrega X_batch y y_batch usando feed_dict en la ejecución\n",
    "            _, minibatch_cost = sess.run([training_op, mse], feed_dict={X: X_batch, y: y_batch})\n",
    "            epoch_cost += minibatch_cost / n_batches\n",
    "        costs.append(epoch_cost)\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta)\n",
    "\n",
    "mse_in_time = [mse_ols] * n_epochs\n",
    "\n",
    "plt.plot(np.squeeze(costs), 'b')\n",
    "plt.plot(mse_in_time, 'r')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compara Resultados (Opcional)\n",
    "Para comparar los resultados entre ambos algoritmos de optimización, ejecuta una sesión para obtener el MSE usando el 'best_theta' obtenido en el último bloque con Descenso del Gradiente por Mini-Batches. También puedes ajustar los hiperparámetros learning rate y/o el número de epochs para la comparación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "y_pred = tf.matmul(X, best_theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"MSE obtenido con Descenso del Gradiente por Mini-Batches:\")\n",
    "    print(mse.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guarda y Restaura tu Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado tu modelo, podrías querer guardar los parámetros obtenidos para luego poder usarlos cuando lo requieras, para usarlos en otro programa, para compararlos con otros modelos, etc. Más aún, podrías querer guardar checkpoints en intervalos regulares de tiempo de manera que si tu computador falla puedas continuar desde el último checkpoit, o para realizar regularización por Early Stopping.\n",
    "\n",
    "En TensorFlow crea un nodo __Saver__ al final de la fase de construcción (luego de que todos los nodos de variables han sido creados), y luego en la fase de ejecución solo llama a su método __save()__ cuando desees, entregándole la sesión y el _path_ del archivo checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\") \n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Define el nodo saver\n",
    "saver = tf.train.Saver([theta])\n",
    "#Usa el siguiente Saver solo si quieres recuperar los valores de theta bajo el nombre \"weigths\"\n",
    "#saver = tf.train.Saver({\"weights\": theta})\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "            #Guarda el nodo saver cada 10 epochs, entregando la sesión y el directorio correspondiente\n",
    "            #con el archivo checkpoint de nombre 'my_model' que será tipo .ckpt\n",
    "            save_path = saver.save(sess, \"./tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    #Guarda el resultado luego de completar el entrenamiento, llamando al archivo ahora 'my_model_final'\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para restaurar un modelo, al comienzo de la fase de ejecución en vez de inicializar las variables usando el nodo _init_, llama al método __restore()__ del objeto _Saver_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()\n",
    "    \n",
    "print('Best theta restored:')\n",
    "print(best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por default, el método __save()__ también guarda la estructura del grafo en un segundo archivo del mismo nombre antes especificado bajo la extensión __.meta__. Puedes cargar este grafo al grafo default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notar que comenzamos con un grafo vacío\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Lo siguiente carga la estructura del grafo\n",
    "saver = tf.train.import_meta_graph(\"./tmp/my_model_final.ckpt.meta\", clear_devices=True)\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Lo siguiente restaura el estado del grafo (i.e., los valores de las variables)\n",
    "    saver.restore(sess, \"./tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()\n",
    "    \n",
    "print('Best theta restored:')\n",
    "print(best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizando el Grafo Computacional ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuta la siguiente celda para crear las funciones que graficarán el grafo computacional creado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que el último grafo que fue cargado fue el grafo restaurado desde el archivo .meta (antes de eso el grafo fue reiniciado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos crear un grafo que calcule dos ReLUs y sume sus outputs. Esto se puede realizar en TensorFlow como sigue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z2, 0., name=\"relu2\")\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código funciona bien, pero es muy repetitivo. Una alternativa mejor es definir una función que realice esta tarea y pueda automáticamente generar todas las ReLUs que necesitemos. TensorFlow se asegura de que los nombres no se repitan, por lo que las ReLUs se irán denominando relu, relu_1, relu_2, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O aún mejor, usando __name scope__ podemos definir los pesos y bias y el resto de las operaciones dentro de un _name scope_ llamado 'relu'. Esto permite otorgar un contexto a ciertas operaciones que definamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0., name=\"max\")\n",
    "    \n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2: Construyendo una Red Neuronal en TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio se construirá una __red neuronal feedforward__ con dos capas para modelar el dataset MNIST, el cual contiene imágenes de dígitos del 0 al 9 escritos a mano. Se mostrará que con un simple modelo la red neuronal es capaz de predecir a qué dígitos corresponden las imágenes nunca vistas antes por nuestra red con altos niveles de precisión!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "X_val = mnist.validation.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_val = mnist.validation.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se ven los datos descargados de MNIST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/nicoaramayo/imgsTFBasics/raw/master/mnist1.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hay',X_train.shape[0],'datos de entrenamiento')\n",
    "print('Hay ',X_val.shape[0],'datos de validación')\n",
    "print('Hay',X_test.shape[0],'datos de testeo')\n",
    "print('Hay  ',X_train.shape[1],'atributos por imagen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/nicoaramayo/imgsTFBasics/raw/master/mnist1.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase de construcción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pixeles MNIST\n",
    "n_inputs = 28*28\n",
    "#Número de unidades ocultas en la primera capa\n",
    "n_hidden1 = 300\n",
    "#Número de unidades ocultas en la segunda capa\n",
    "n_hidden2 = 100\n",
    "#Número de clases (particular para MNIST)\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Los parámetros de shape para X son None porque no sabemos apriori el tamaño de los batchs (y pueden ser variables),\n",
    "#y n_inputs es fijo (cada atributo corresponde a un pixel)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos la red neuronal. El placeholder de X funcionará como la capa de input, y resta crear las dos capas escondidas más la capa de output. También se debe definir la función de costos y el algoritmo de optimización. Luego, definiremos los name scopes:<br>\n",
    "> -__dnn__: red neuronal <br>\n",
    "> -__loss__: función objetivo <br>\n",
    "> -__train__: entrenamiento <br>\n",
    "> -__eval__: obtiene métricas de desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    #El argumento 'logits' son las log probabilidades no escaladas definidas en la parte anterior\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    #tf.nn.in_top_k me dice si mis targets están en las primeras k predicciones. En este caso las predicciones son\n",
    "    #'logits', los targets son 'y', y k=1. Luego, retorna un tensor con booleans\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    #tf.cast pasa un tensor a otro tipo de dato, en este caso, tf.float32. Luego calculamos la media entre los 0s (errores)\n",
    "    #y 1s (aciertos)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El grafo construido es una red neuronal feedforward (o _fully connected_) como se muestra en la imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/nicoaramayo/imgsTFBasics/raw/master/ffnn.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            #Ya está implementado obtener los batches con .next_batch(batch_size)\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                            y: mnist.validation.labels})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego del entrenamiento, podemos ir revisando qué es lo que predice nuestra red neuronal en el set de testeo restaurando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    X_new_scaled = mnist.test.images\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos ahora los resultados en una matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if normalize:\n",
    "        cm = 100 * cm / cm.sum(1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.0f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular la matriz de confusión\n",
    "cnf_matrix = confusion_matrix(mnist.test.labels, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = range(10)\n",
    "\n",
    "#Plotear la matriz de confusión no-normalizada\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "#Plotear la matriz de confusión normalizada\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este tutorial ya manejas los conceptos más importantes para poder desarrollar tus propios modelos TensorFlow, incluyendo poder implementar una red neuronal en el clásico dataset MNIST! <br>\n",
    "Los conceptos que ya manejas son:\n",
    "> + Crear __grafos computacionales__ <br>\n",
    "> + Manejar múltiples grafos computacionales <br>\n",
    "> + Dentro de los grafos: <br>\n",
    ">> + Crear __variables__ y operaciones <br>\n",
    ">> + Iniciar __sesiones__ <br>\n",
    ">> + Y ejecutarlas <br>\n",
    "> + Implementar __descenso del gradiente__ y __descenso del gradiente por minibatches__ (manejando nodos __placeholders__) <br>\n",
    "> + __Guardar__ y __restaurar__ modelos <br>\n",
    "> + Organizar tu fase de construcción en secciones bajo __name scopes__ <br>\n",
    "> + Implementar el grafo computacional para una __red neuronal__, entrenarla, y calcular las métricas de ajuste para evaluarla <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
